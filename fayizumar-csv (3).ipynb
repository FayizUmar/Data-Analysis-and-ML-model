{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-04-08T17:11:09.432824Z","iopub.execute_input":"2023-04-08T17:11:09.433209Z","iopub.status.idle":"2023-04-08T17:11:09.865184Z","shell.execute_reply.started":"2023-04-08T17:11:09.433174Z","shell.execute_reply":"2023-04-08T17:11:09.864030Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:39:57.522860Z","iopub.execute_input":"2023-04-08T19:39:57.524256Z","iopub.status.idle":"2023-04-08T19:39:58.095519Z","shell.execute_reply.started":"2023-04-08T19:39:57.524187Z","shell.execute_reply":"2023-04-08T19:39:58.094314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fill in the missing code\nThe following script is an implementation of a logistic regression model for a binary classification problem. The dataset contains four numerical input attributes and one output attribute (class label 0 or 1).\n\n- Columns **Attribute1**, **Attribute2**, **Attribute3**, and **Attribute4** are inputs.\n- Column **OutputClass** is output.\n\nThe missing pieces of code are indicated like so: `#write your code here#`.\n\nThe functions corresponding to said missing code are stated in the description above each function.\n\nTo complete the task, replace `#write your code here#` with the your own code. Also, ensure that the program executes without any warnings or errors.","metadata":{"execution":{"iopub.status.busy":"2023-04-08T12:45:55.326416Z","iopub.execute_input":"2023-04-08T12:45:55.326840Z","iopub.status.idle":"2023-04-08T12:45:55.337444Z","shell.execute_reply.started":"2023-04-08T12:45:55.326802Z","shell.execute_reply":"2023-04-08T12:45:55.335345Z"}}},{"cell_type":"code","source":"# Reading CSV file as pandas dataframe\ndata = pd.read_csv(\"/kaggle/input/summer-internship-cistup-iisc-2023/Full_Data.csv\")\n\ndata_X = data.values[:, :-1]\n\ndata_Y = data.values[:, -1].astype(\"int\")\n\n# Splitting dataset into training and test set\nX_full, test_X_full, Y, test_Y = train_test_split(data_X, data_Y, test_size=0.4, shuffle=True, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:40:01.557271Z","iopub.execute_input":"2023-04-08T19:40:01.557664Z","iopub.status.idle":"2023-04-08T19:40:01.587115Z","shell.execute_reply.started":"2023-04-08T19:40:01.557631Z","shell.execute_reply":"2023-04-08T19:40:01.585817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try out different input column indexes as input data. For instance, if you choose to use input columns 2 and 3; replace `#write your code here#` with `1, 2` (resp.) in the code block below.\n\nExample:\n* `attr1 = 1`\n* `attr2 = 2`\n\nNote column 1 (i.e., column index = 0) is `RowIdx`, and should not be used as an input.\n\nAlso note, *column index = column number - 1*.","metadata":{}},{"cell_type":"code","source":"# Removing all columns other than two\nattr1 = 2\nattr2 = 3\nX = X_full[:, (attr1, attr2)]\ntest_X = test_X_full[:, (attr1, attr2)]","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:08.629377Z","iopub.execute_input":"2023-04-08T19:51:08.629816Z","iopub.status.idle":"2023-04-08T19:51:08.636311Z","shell.execute_reply.started":"2023-04-08T19:51:08.629779Z","shell.execute_reply":"2023-04-08T19:51:08.634826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ploting attributes\nplt.plot(X[:, 0][Y==0], X[:, 1][Y==0], \"o\")\nplt.plot(X[:, 0][Y==1], X[:, 1][Y==1], \"s\")\nplt.xlabel(\"Attribute 1\")\nplt.ylabel(\"Attribute 2\")","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:09.328637Z","iopub.execute_input":"2023-04-08T19:51:09.329305Z","iopub.status.idle":"2023-04-08T19:51:09.518912Z","shell.execute_reply.started":"2023-04-08T19:51:09.329266Z","shell.execute_reply":"2023-04-08T19:51:09.517564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code block below constructs $\\mathbf{X}$. Note that $\\mathbf{X}$ includes $x_0 = 1$ column for the bias (i.e., intercept).","metadata":{}},{"cell_type":"code","source":"# Combining input data with fabricated output class\nX = np.hstack((np.ones((X.shape[0], 1)), X))\nprint(\"The regenerated input data has class labels set to 1:\\n\", X)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:10.778819Z","iopub.execute_input":"2023-04-08T19:51:10.779255Z","iopub.status.idle":"2023-04-08T19:51:10.786298Z","shell.execute_reply.started":"2023-04-08T19:51:10.779216Z","shell.execute_reply":"2023-04-08T19:51:10.785392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic Regression Model\n\nFor Logistic Regression, our hypothesis is \n$$\n\\hat{Y} = h_w(x) = \\frac{1}{1+e^{-(w^{T}x)}}\n$$\nThe output range of $\\hat{Y}$ is between 0 and 1.","metadata":{}},{"cell_type":"markdown","source":"### Sigmoid Function\n\nThe Sigmoid function squishes all its inputs (i.e., values on x-axis) between 0 and 1.\n$$\n\\sigma(z) = \\frac{1}{1+e^{-z}}\n$$","metadata":{}},{"cell_type":"code","source":"# Defining sigmoid function\ndef sigmoid(z):\n    # z --> input\n    # sigmoid_z --> output of sigmoid function\n    z = z.astype(float)\n    sigmoid_z = 1/(1+np.exp(-z))\n    \n    return sigmoid_z\n\n#sample output\nz=30\nprint_sig=sigmoid(np.array([z]))\nprint(\"sigmoid of\",z,\"is\",print_sig)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:12.409020Z","iopub.execute_input":"2023-04-08T19:51:12.409449Z","iopub.status.idle":"2023-04-08T19:51:12.416657Z","shell.execute_reply.started":"2023-04-08T19:51:12.409411Z","shell.execute_reply":"2023-04-08T19:51:12.415599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The cost function for Logistic Regression for binary classification:\n$$\nJ(data, w) = \\frac{1}{n}\\sum_{i=1}^{n} L(\\hat{Y}^{(i)},Y^{(i)}) = -\\frac{1}{n}\\sum_{i=1}^{n} [Y^{(i)}log(\\hat{Y}^{(i)}) + (1-Y^{(i)})log(1-\\hat{Y}^{(i)})]\n$$\n\nThis loss is also called binary cross entropy error.","metadata":{}},{"cell_type":"code","source":"# Defining loss function\ndef loss(Y, y_hat):\n    # Y --> data|\n    # y_hat --> w\n    loss_value = np.mean(Y * np.log(sigmoid(y_hat)) + (1 - Y) * np.log(1 - sigmoid(y_hat)))\n    \n    return -loss_value\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:13.088789Z","iopub.execute_input":"2023-04-08T19:51:13.089942Z","iopub.status.idle":"2023-04-08T19:51:13.096815Z","shell.execute_reply.started":"2023-04-08T19:51:13.089887Z","shell.execute_reply":"2023-04-08T19:51:13.095350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gradient of the loss function\n\nUsing the Gradient Descent Algorithm, optimal values of the parameters can be calculated like so ($\\eta$ →learning rate), the update rules for parameters are as follows:\n$$\nw_{t+1} = w_{t} - \\eta*dw\n$$\nWhere $dw$ is the partial derivative of loss w.r.t parameter $w$. It looks like:\n$$\ndw = \\frac{1}{n} * (\\hat{y}-y).\\textbf{X}\n$$","metadata":{}},{"cell_type":"code","source":"# Defining gradient function\ndef gradients(X, Y, y_hat):\n    # X --> input\n    # Y --> true/target value\n    # y_hat --> hypothesis/predictions\n    # n --> number of training examples\n    \n    n = X.shape[0]\n    \n    # Gradient of loss w.r.t weights\n    dw = (1/n) * np.dot((y_hat - Y).X)\n    \n    \n    return dw","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:13.888496Z","iopub.execute_input":"2023-04-08T19:51:13.888908Z","iopub.status.idle":"2023-04-08T19:51:13.896233Z","shell.execute_reply.started":"2023-04-08T19:51:13.888870Z","shell.execute_reply":"2023-04-08T19:51:13.894567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Normalize the data before using/computing gradient. It can accelerate the training process. Make sure you don\"t normalize the \"bias\" term (i.e., first column).","metadata":{}},{"cell_type":"code","source":"# Defining data normalization function\ndef normalize(X):\n    # X --> input\n    # n --> number of training examples\n    # d --> number of features \n    n, d = X.shape\n    \n    # Normalizing all the d features of X (except the bias (first) column)\n    for i in range(d-1):\n        X[:,i+1] = (X[:,i+1] - X[:,i+1].mean(axis=0))/X[:,i+1].std(axis=0)\n        \n    return X","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:14.638851Z","iopub.execute_input":"2023-04-08T19:51:14.639390Z","iopub.status.idle":"2023-04-08T19:51:14.647067Z","shell.execute_reply.started":"2023-04-08T19:51:14.639349Z","shell.execute_reply":"2023-04-08T19:51:14.645681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prediction\n\nNow that the functions to learn the parameters are ready, check if the hypothesis ($\\hat{Y}$) is able to predict the output class $Y=1$ or $Y=0$. Note that the hypothesis is the probability of $Y$ being 1 given $\\textbf{X}$ and is parameterized by $w$.\n\nHence, the prediction function will be so —\n$$\n\\hat{Y} = 1 \\to w^{T}\\textbf{X}\n\\geq 0\n$$\n$$\n\\hat{Y} = 0  \\to w^{T}\\textbf{X} < 0\n$$","metadata":{}},{"cell_type":"code","source":"# Defining prediction function\ndef predict(X,w):\n    # X --> Input.\n    \n    # Normalizing the inputs.\n    X = normalize(X)\n    \n    # Calculating prediction/y_hat.\n    preds = sigmoid(np.dot(X, w))\n \n    \n    # Empty List to store predictions.\n    \n    pred_class=[]\n    for pred in preds:\n        if pred >= 1/2:\n            pred_class.append(1)\n        else:\n            pred_class.append(0)\n    \n    return np.array(pred_class)\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:16.038538Z","iopub.execute_input":"2023-04-08T19:51:16.038945Z","iopub.status.idle":"2023-04-08T19:51:16.046249Z","shell.execute_reply.started":"2023-04-08T19:51:16.038907Z","shell.execute_reply":"2023-04-08T19:51:16.044951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The decision boundary will be:\n$$\n\\hat{Y} = 0.5 \\quad or \\quad w^{T}\\textbf{X} = 0\n$$","metadata":{}},{"cell_type":"code","source":"# Defining function to plot decision boundary\ndef plot_decision_boundary(X,w):\n    ydisp = -(w[0] + w[1] * X)/w[2]\n    \n    fig = plt.figure(figsize=(10, 8))\n    plt.plot(X[:, 1][Y==0], X[:, 2][Y==0], \"^\")\n    plt.plot(X[:, 1][Y==1], X[:, 2][Y==1], \"s\")\n    \n    plt.xlim([-2, 5])\n    plt.ylim([-2, 5])\n    plt.xlabel(\"Attribute 1\")\n    plt.ylabel(\"Attribute 2\")\n    plt.title(\"Decision Boundary\")\n    plt.plot(X, ydisp)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:16.838725Z","iopub.execute_input":"2023-04-08T19:51:16.839584Z","iopub.status.idle":"2023-04-08T19:51:16.847451Z","shell.execute_reply.started":"2023-04-08T19:51:16.839538Z","shell.execute_reply":"2023-04-08T19:51:16.846035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now that all the required blocks for logistic regression model are ready, encode the model.","metadata":{}},{"cell_type":"code","source":"# Defining training function\ndef train(X, Y, epochs, eta):\n    # X --> input\n    # Y --> true/target value\n    # bs --> batch size\n    # eta --> learning rate\n    # n-> number of training examples\n    # d-> number of features \n    \n    n, d = X.shape\n    \n    # Initializing weights and bias to zeros\n    w = np.zeros((d,1))\n    \n    # Reshaping Y\n    Y = Y.reshape(n,1)\n    \n    # Normalizing the inputs\n    X = normalize(X)\n    \n    # Empty list to store losses\n    losses = []\n    \n    # Training loop\n    for epoch in range(epochs):\n        \n            # Calculating hypothesis/prediction\n            y_hat = sigmoid(np.dot(X, w))\n            \n            # Getting the gradients of loss w.r.t parameters\n            dw =  (1/n) * np.dot(X.T, (y_hat - Y))\n            # Updating the parameters.\n            w = w- eta*dw\n            \n            # Calculating loss and appending it in the list\n            l = loss(Y,y_hat)\n            losses.append(l)\n        \n    # Returning weights, losses(List)\n    return w, losses","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:19.229319Z","iopub.execute_input":"2023-04-08T19:51:19.229765Z","iopub.status.idle":"2023-04-08T19:51:19.238479Z","shell.execute_reply.started":"2023-04-08T19:51:19.229724Z","shell.execute_reply":"2023-04-08T19:51:19.237089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train the model and print the results.\n\n#Try out different learning rates to improve model performance.\n\n#Example: w, l = train(X, Y, epochs=100, eta=0.001)`","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:19.749055Z","iopub.execute_input":"2023-04-08T19:51:19.749498Z","iopub.status.idle":"2023-04-08T19:51:19.755527Z","shell.execute_reply.started":"2023-04-08T19:51:19.749462Z","shell.execute_reply":"2023-04-08T19:51:19.753831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Training model \nw, l = train(X, Y, epochs=100, eta=0.001)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:21.669585Z","iopub.execute_input":"2023-04-08T19:51:21.670123Z","iopub.status.idle":"2023-04-08T19:51:21.754868Z","shell.execute_reply.started":"2023-04-08T19:51:21.670080Z","shell.execute_reply":"2023-04-08T19:51:21.752888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting loss vs. epoch function\nplt.plot(l)\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:22.513881Z","iopub.execute_input":"2023-04-08T19:51:22.514682Z","iopub.status.idle":"2023-04-08T19:51:22.662328Z","shell.execute_reply.started":"2023-04-08T19:51:22.514642Z","shell.execute_reply":"2023-04-08T19:51:22.661088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing training accuracy\nprint(\"The accuracy of model is\",(np.sum(1*(Y==predict(X,w)))/len(Y))*100,\"%\")","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:23.868589Z","iopub.execute_input":"2023-04-08T19:51:23.869832Z","iopub.status.idle":"2023-04-08T19:51:23.895417Z","shell.execute_reply.started":"2023-04-08T19:51:23.869789Z","shell.execute_reply":"2023-04-08T19:51:23.893684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the decision boundary\nplot_decision_boundary(X, w)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:24.378590Z","iopub.execute_input":"2023-04-08T19:51:24.378997Z","iopub.status.idle":"2023-04-08T19:51:24.677460Z","shell.execute_reply.started":"2023-04-08T19:51:24.378962Z","shell.execute_reply":"2023-04-08T19:51:24.676465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_X.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:51:26.339025Z","iopub.execute_input":"2023-04-08T19:51:26.339643Z","iopub.status.idle":"2023-04-08T19:51:26.344641Z","shell.execute_reply.started":"2023-04-08T19:51:26.339606Z","shell.execute_reply":"2023-04-08T19:51:26.343772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Run the test data through the trained model, and print the testing accuracy.","metadata":{}},{"cell_type":"code","source":"# Checking test accuracy\ntest_X = np.hstack((np.ones((test_X.shape[0],1)), test_X))\nml_predictions = predict(test_X,w)\nprint(\"The test accuracy of model is\",(np.sum(1*(test_Y==ml_predictions))/len(test_Y))*100,\"%\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:52:18.379497Z","iopub.execute_input":"2023-04-08T19:52:18.379949Z","iopub.status.idle":"2023-04-08T19:52:18.391577Z","shell.execute_reply.started":"2023-04-08T19:52:18.379910Z","shell.execute_reply":"2023-04-08T19:52:18.390205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Exporting results\nrow_idx = np.concatenate((test_X_full[:, 0].astype(int), X_full[:, 0].astype(int)))\nopt_cls = np.concatenate((ml_predictions.astype(int), Y.astype(int)))\n                              \noutput_data = np.concatenate((row_idx.reshape(-1,1), opt_cls.reshape(-1,1)), axis=1)\npd.DataFrame(output_data, columns=[\"RowIdx\", \"OutputClass\"]).to_csv(\"Output_Data.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-08T19:52:48.490075Z","iopub.execute_input":"2023-04-08T19:52:48.491158Z","iopub.status.idle":"2023-04-08T19:52:48.512707Z","shell.execute_reply.started":"2023-04-08T19:52:48.491115Z","shell.execute_reply":"2023-04-08T19:52:48.511353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}